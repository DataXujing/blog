{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# course2 week1 笔记\n",
    "---\n",
    "内容概要：\n",
    "* 训练集、验证集、测试集（train、dev、test sets）的意义以及如何划分\n",
    "* 偏差和方差（bias and variance）的区别，如何处理高偏差、高方差以及两者共存的问题\n",
    "* 在神经网络中应用正则化方法降低过拟合风险，如：L2正则化、dropout\n",
    "* 一些加速神经网络训练的方法，如：Normalizing inputs\n",
    "* 梯度校验（Gradient Checking）— 一种网络不起作用时的debug方法\n",
    "\n",
    "\n",
    "\n",
    "* 正则化（Regularization）\n",
    "    \n",
    "    How does regularization prevent overfitting?\n",
    "    dropout\n",
    "    data augmentation\n",
    "    early stopping\n",
    "    \n",
    "* Gradient Checking\n",
    "* Vanishing/exploding gradients\n",
    "* Normalizing inputs\n",
    "\n",
    "\n",
    "正则化 L1范数、L2范数\n",
    "Logistic Regression中的正则化\n",
    "神经网络中的正则化\n",
    "\n",
    "为什么正则化可以降低过拟合的风险？\n",
    "> L2正则能够降低部分神经元的权重，从而事实上简化了网络。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "---\n",
    "There are two types of parameters to initialize in a neural network:\n",
    "- the weight matrices $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "- the bias vectors $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n",
    "\n",
    "- *Zeros initialization* --  setting `initialization = \"zeros\"` in the input argument.\n",
    "- *Random initialization* -- setting `initialization = \"random\"` in the input argument. This initializes the weights to large random values.  \n",
    "- *He initialization* -- setting `initialization = \"he\"` in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015. \n",
    "\n",
    "\"He Initialization\"; this is named for the first author of He et al., 2015. (If you have heard of \"Xavier initialization\", this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of `sqrt(1./layers_dims[l-1])` where He initialization would use `sqrt(2./layers_dims[l-1])`.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "---\n",
    "The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Let's modify your cost and observe the consequences.\n",
    "\n",
    "**Exercise**: Implement `compute_cost_with_regularization()` which computes the cost given by formula (2). To calculate $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$  , use :\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "\n",
    "**Observations**:\n",
    "- The value of $\\lambda$ is a hyperparameter that you can tune using a dev set.\n",
    "- L2 regularization makes your decision boundary smoother. If $\\lambda$ is too large, it is also possible to \"oversmooth\", resulting in a model with high bias.\n",
    "\n",
    "**What is L2-regularization actually doing?**:\n",
    "\n",
    "L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. \n",
    "\n",
    "<font color='blue'>\n",
    "**What you should remember** -- the implications of L2-regularization on:\n",
    "- The cost computation:\n",
    "    - A regularization term is added to the cost\n",
    "- The backpropagation function:\n",
    "    - There are extra terms in the gradients with respect to weight matrices\n",
    "- Weights end up smaller (\"weight decay\"): \n",
    "    - Weights are pushed to smaller values.\n",
    "\n",
    "\n",
    "\n",
    "Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system. \n",
    "- Regularization will help you reduce overfitting.\n",
    "- Regularization will drive your weights to lower values.\n",
    "- L2 regularization and Dropout are two very effective regularization techniques.\n",
    "\n",
    "**dropout** is a widely used regularization technique that is specific to deep learning. \n",
    "**It randomly shuts down some neurons in each iteration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程作业中的知识点\n",
    "---\n",
    "第一份作业：\n",
    "参数初始化方法对神经网络的影响\n",
    "\n",
    "第二份作业：\n",
    "正则化方法降低过拟合风险\n",
    "\n",
    "第三份作业：\n",
    "梯度校验 — 网络不起作用时的debug方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropout（随机失活）\n",
    "---\n",
    "随机选择一些神经元，删掉，得到简化的神经网络。\n",
    "\n",
    "不能依赖任何特征，因为任何特征都有可能被清除。\n",
    "\n",
    "在计算机视觉中常用，防止过拟合。\n",
    "\n",
    "Why does drop-out work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他正则化方法\n",
    "---\n",
    "数据增强（data augmentation）-- 图像旋转、缩减\n",
    "\n",
    "early stopping\n",
    "提前停止迭代，决策依据是训练集误差曲线和测试集误差曲线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Training Data\n",
    "---\n",
    "subtract mean\n",
    "normalize variou\n",
    "\n",
    "意义：让训练更快，不容易错过最优值。\n",
    "\n",
    "\n",
    "### Vanishing / Exploding Gradients 梯度消失，梯度爆炸\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization for deep networks\n",
    "---\n",
    "\n",
    "### 梯度的数值逼近\n",
    "---\n",
    "\n",
    "### 梯度检验 Gradient Checking -- debug神经网络的有效方法\n",
    "---\n",
    "1、不要再训练过程中进行梯度检验，仅仅在debug过程中使用\n",
    "2、\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## \n",
    "\n",
    "[关于梯度消失，梯度爆炸的问题](http://blog.csdn.net/qq_29133371/article/details/51867856)\n",
    "[梯度爆炸和梯度消失的本质原因](http://blog.csdn.net/lujiandong1/article/details/53320174)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
