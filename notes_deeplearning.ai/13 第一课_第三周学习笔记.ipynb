{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第一课_第三周学习笔记\n",
    "---\n",
    "内容概要：\n",
    "* 四种常用的激活函数、导数，以及为什么需要非线性激活函数\n",
    "\n",
    "* 参数随机初始化，为什么不能将所有参数初始化为0\n",
    "\n",
    "* 单隐层神经网络简介\n",
    "\n",
    "* 分步骤实现单隐层神经网络（forward propagation & back propagation）\n",
    "\n",
    "* tensorflow 中实现单隐层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - 符号说明\n",
    "---\n",
    "\n",
    "|符号|含义|\n",
    "|:-----:|:----|\n",
    "|$X$|堆叠（stack）所有输入样本得到的输入矩阵|\n",
    "|$x^{(i)}$|数据集中的第 $i$ 个样本的输入向量|\n",
    "|$Y$|堆叠（stack）之后的真实值（标签）向量|\n",
    "|$y^{(i)}$|第 $i$ 个样本的真实值（标签）|\n",
    "|$\\hat{y}^{(i)}$|第 $i$ 个样本的最终预测值|\n",
    "|$a^{(i)}$|训练过程中，第 $i$ 个样本的预测值|\n",
    "|$A$|训练过程中，所有样本的预测值矩阵|\n",
    "|$m$|输入样本数量|\n",
    "|$n$|每一个样本的特征数量|\n",
    "|$w_i$|第 $i$ 个特征的权重系数|\n",
    "|$W$|所有特征的权重系数向量|\n",
    "|$b$|线性回归中的截断误差|\n",
    "|$Z$|神经元中的线性计算结果矩阵，激活函数的输入|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 激活函数\n",
    "---\n",
    "激活函数是神经网络中非常关键的一个组成部分，常用的激活函数都是非线性的，如sigmoid、tanh、ReLU、Leaky_ReLU等。四种常用激活函数的图像如下：\n",
    "\n",
    "<img src=\"images/activation.jpg\" style=\"width:600px\">\n",
    "\n",
    ">**为什么要使用非线性激活函数？**\n",
    "\n",
    ">如果不使用非线性激活函数，那个神经网络仅仅是将输入线性组合后输出，而两个线性方程的组合依然是线性方程。这样的话，神经网络就没法逼近复杂的非线性函数。当然，在一些比较特殊的问题中，如回归等，也会使用线性激活函数。\n",
    "\n",
    "### 1.1 - sigmoid & tanh\n",
    "---\n",
    "[sigmoid][01] 常用于二分类（binary classification）网络的输出层；在隐藏层中，通常使用 [tanh][02]，而不是sigmoid。这主要是因为tanh的输出为（-1，1），均值为0，所以下一层的输入是centered；而sigmoid的均值是0.5。tanh其实就是sigmoid在y轴方向向下移动1个单位的版本。\n",
    "\n",
    "sigmoid函数表达式及其导数如下：\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}\\tag{sigmoid函数}$$\n",
    "$$f(x)^{'} = f(x)(1-f(x))\\tag{sigmoid导数}$$\n",
    "\n",
    "$$f(x) = tanh(x)\\tag{tanh函数}$$\n",
    "\n",
    "$$f(x)^{'} = 1-f(x)^{2}\\tag{tanh导数}$$\n",
    "\n",
    "### 1.2 - ReLU & Leaky_ReLU\n",
    "---\n",
    "[ReLU (Rectified Linear Unit)][03]激活函数在深度学习中比sigmoid和tanh更常用。\n",
    "当激活函数的输入z的值很大或很小的时候，sigmoid和tanh的梯度非常小，这会大大减缓梯度下降的学习速度。所以与sigmoid和tanh相比，ReLU的训练速度要快很多。Leaky ReLU比ReLU要表现得稍微好一丢丢但是实践中大家往往都用ReLU。\n",
    "\n",
    "$$f(x) = max(0, x)\\tag{ReLU函数}$$\n",
    "\n",
    "\n",
    "[01]:https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "[02]:https://en.wikipedia.org/wiki/Hyperbolic_function\n",
    "[03]:https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 神经元\n",
    "---\n",
    "典型的神经元如下图所示：\n",
    "<img src=\"images/nn1.jpg\" style=\"width:300px\">\n",
    "\n",
    "神经元中包含两个计算过程：\n",
    "\n",
    "step1. 由线性方程计算输入向量 $x$ 的线性回归值,即 $z=w^Tx+b$ ；\n",
    "\n",
    "step2. 将线性回归值输入激活函数 $\\alpha=\\sigma(z)$，得到结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 参数随机初始化\n",
    "---\n",
    "\n",
    "对于Logistic Regression，将参数初始化为0没有问题；但是，在神经网络中，如果将所有参数初始化为0，再使用梯度下降算法训练，就会完全无效。因为，所有参数初始化为0之后，单个隐层中的所有神经元每一次迭代的计算内容就完全一样了，参数更新也一样，也就是说参数矩阵的所有元素都是一样的，这样就无法达到训练的目的。\n",
    "\n",
    "随机初始化参数可以解决以上问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blog — 个人博客仓库\n",
    "\n",
    ">blog是我的博客仓库，主要是存放一些博客写作想法和草稿，当然，写好的博客也会在这里有一个备份，以便持续完善。blog正式发布地址 [zengbin-简书](http://www.jianshu.com/u/0cd3889d64bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单隐层神经网络简介\n",
    "---\n",
    "将若干个神经元分层组合，就是神经网络。典型的单隐层神经网络结构如下图：\n",
    "<img src=\"images/nn4.jpg\" style=\"width:300px\">\n",
    "其中，红色覆盖的是【输入层】，黄色覆盖的是【隐含层】，绿色覆盖的是【输出层】。\n",
    "\n",
    "上图中，隐层中有4个神经元，神经元数量可以根据任务的需求调整。\n",
    "\n",
    "由于输入层没有任何计算，仅仅是数据的输入，在计算神经网络的层数时，通常不考虑输入层。因此，单隐层神经网络的层数是2。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单隐层神经网络的实现过程\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Reminder**: The general methodology to build a Neural Network is to:\n",
    "    1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "    2. Initialize the model's parameters\n",
    "    3. Loop:\n",
    "        - Implement forward propagation\n",
    "        - Compute loss\n",
    "        - Implement backward propagation to get the gradients\n",
    "        - Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单隐层神经网络中的梯度下降\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"images/grad_summary.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "- Tips:\n",
    "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
    "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`.\n",
    "\n",
    "---\n",
    "**gradient descent参数更新规则**: \n",
    "$$ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$$\n",
    "其中：$\\alpha$ 是学习率（learning rate）；$\\theta$ 是需要更新的参数\n",
    "\n",
    "---\n",
    "**训练过程好坏的辨别**: \n",
    "\n",
    "好的训练过程：\n",
    "<img src=\"images/sgd.gif\" style=\"width:200px\"> \n",
    "差的训练过程：\n",
    "<img src=\"images/sgd_bad.gif\" style=\"width:200px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单层神经网络的实现\n",
    "---\n",
    "\n",
    "1. ng提供的课程作业中的实现\n",
    "2. TensorFlow中实现单层神经网络\n",
    "\n",
    "Tuning hidden layer size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow中单隐层神经网络的实现\n",
    "---\n",
    "使用ng在本周编程作业中提供的数据，在TensorFlow中实现单隐层神经网络如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D), dtype='float32') # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.714734\n",
      "Cost after iteration 1000: 0.336571\n",
      "Cost after iteration 2000: 0.318074\n",
      "Cost after iteration 3000: 0.309604\n",
      "Cost after iteration 4000: 0.304108\n",
      "Cost after iteration 5000: 0.299986\n",
      "Cost after iteration 6000: 0.296653\n",
      "Cost after iteration 7000: 0.293828\n",
      "Cost after iteration 8000: 0.291358\n",
      "Cost after iteration 9000: 0.289151\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 加载数据\n",
    "X, Y = load_planar_dataset()\n",
    "\n",
    "# 定义神经网络结构\n",
    "n_x, n_h, n_y = X.shape[0], 4, Y.shape[0]\n",
    "m = X.shape[1]\n",
    "\n",
    "# 参数初始化\n",
    "W1 = tf.Variable(tf.random_normal([n_h, n_x], stddev=1, seed=1))\n",
    "b1 = tf.Variable(tf.zeros([n_h, 1]))\n",
    "W2 = tf.Variable(tf.random_normal([n_y, n_h], stddev=1, seed=1))\n",
    "b2 = tf.Variable(tf.zeros([n_y, 1]))\n",
    "\n",
    "# 数据输入\n",
    "x = tf.placeholder(tf.float32, shape=(n_x, None), name='x-input')\n",
    "y_ = tf.placeholder(tf.float32, shape=(n_y, None), name='y-input')\n",
    "\n",
    "# 前向传播\n",
    "z1 = tf.matmul(W1, X) + b1\n",
    "a1 = tf.nn.tanh(z1)\n",
    "z2 = tf.matmul(W2, a1) + b2\n",
    "a2 = tf.nn.sigmoid(z2)\n",
    "\n",
    "# 成本函数 & 训练图\n",
    "cost = - tf.reduce_sum(tf.add(tf.multiply(y_ , tf.log(a2)), tf.multiply(1-y_ ,tf.log(1-a2)))) / m\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cost)  # 0.1 为 learning_rate\n",
    "# train_step = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "# 创建 Session，运行\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # 全局参数初始化\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 训练模型\n",
    "    num_iterations = 10000\n",
    "    for i in range(num_iterations):\n",
    "        sess.run(train_step, feed_dict={x: X, y_: Y})\n",
    "        if i % 1000 == 0:\n",
    "            total_cost = sess.run(cost, feed_dict={x: X, y_: Y})\n",
    "            print (\"Cost after iteration %i: %f\" %(i, total_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "---\n",
    "[吴恩达Coursera Deep Learning学习笔记 1 （下）](http://www.jianshu.com/p/51a5ff911c41)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
